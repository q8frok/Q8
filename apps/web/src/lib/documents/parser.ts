/**
 * Document Parser
 * Parses different file types into text chunks for embedding
 */

import type { FileType, ParsedDocument, ParsedChunk, ChunkType } from './types';
import { logger } from '@/lib/logger';

/**
 * Chunk size configuration
 */
const CHUNK_CONFIG = {
  maxChunkSize: 1000,      // Max characters per chunk
  chunkOverlap: 200,       // Overlap between chunks for context
  minChunkSize: 100,       // Minimum chunk size
  codeChunkSize: 500,      // Smaller chunks for code
};

/**
 * Detect file type from MIME type and extension
 */
export function detectFileType(mimeType: string, fileName: string): FileType {
  const ext = fileName.split('.').pop()?.toLowerCase() || '';

  // Check by MIME type first
  if (mimeType === 'application/pdf') return 'pdf';
  if (mimeType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') return 'docx';
  if (mimeType === 'application/msword') return 'doc';
  if (mimeType === 'text/plain') return ext === 'md' ? 'md' : 'txt';
  if (mimeType === 'text/markdown') return 'md';
  if (mimeType === 'text/csv') return 'csv';
  if (mimeType === 'application/json') return 'json';
  if (mimeType === 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet') return 'xlsx';
  if (mimeType === 'application/vnd.ms-excel') return 'xls';
  if (mimeType === 'application/vnd.openxmlformats-officedocument.presentationml.presentation') return 'pptx';
  if (mimeType === 'application/vnd.ms-powerpoint') return 'ppt';
  if (mimeType.startsWith('image/')) return 'image';

  // Check by extension for code files
  const codeExtensions = [
    'js', 'ts', 'jsx', 'tsx', 'py', 'rb', 'go', 'rs', 'java', 'c', 'cpp', 'h',
    'cs', 'php', 'swift', 'kt', 'scala', 'sh', 'bash', 'zsh', 'sql', 'html',
    'css', 'scss', 'sass', 'less', 'vue', 'svelte', 'yaml', 'yml', 'toml',
    'xml', 'graphql', 'prisma', 'tf', 'dockerfile',
  ];
  if (codeExtensions.includes(ext)) return 'code';

  // Check extension as fallback
  const extMap: Record<string, FileType> = {
    pdf: 'pdf',
    docx: 'docx',
    doc: 'doc',
    txt: 'txt',
    md: 'md',
    csv: 'csv',
    json: 'json',
    xlsx: 'xlsx',
    xls: 'xls',
    pptx: 'pptx',
    ppt: 'ppt',
  };

  return extMap[ext] || 'other';
}

/**
 * Parse a document based on its file type
 */
export async function parseDocument(
  content: ArrayBuffer | string,
  fileType: FileType,
  fileName: string
): Promise<ParsedDocument> {
  switch (fileType) {
    case 'pdf':
      return parsePDF(content as ArrayBuffer);
    case 'docx':
    case 'doc':
      return parseDOCX(content as ArrayBuffer);
    case 'image':
      return parseImage(content as ArrayBuffer);
    case 'txt':
    case 'md':
      return parseText(content as string, fileType);
    case 'csv':
      return parseCSV(content as string);
    case 'json':
      return parseJSON(content as string);
    case 'code':
      return parseCode(content as string, fileName);
    case 'xlsx':
    case 'xls':
      return parseExcel(content as ArrayBuffer);
    case 'pptx':
    case 'ppt':
      return parsePPTX(content as ArrayBuffer);
    default:
      // Try to parse as text
      if (typeof content === 'string') {
        return parseText(content, 'txt');
      }
      throw new Error(`Unsupported file type: ${fileType}`);
  }
}

/**
 * Parse PDF document
 */
async function parsePDF(content: ArrayBuffer): Promise<ParsedDocument> {
  try {
    // Dynamic import for PDF parsing
    const pdfParseModule = await import('pdf-parse');
    // Handle both ESM and CJS exports
    const pdfParse = (pdfParseModule as { default?: (buffer: Buffer) => Promise<{ text: string; numpages: number; info: Record<string, unknown> }> }).default
      ?? (pdfParseModule as unknown as (buffer: Buffer) => Promise<{ text: string; numpages: number; info: Record<string, unknown> }>);
    const buffer = Buffer.from(content);
    const data = await pdfParse(buffer);

    const chunks = chunkText(data.text, 'text');

    // Add page information if available
    const pageChunks: ParsedChunk[] = chunks.map((chunk, index) => ({
      ...chunk,
      sourcePage: Math.floor((index / chunks.length) * (data.numpages || 1)) + 1,
    }));

    return {
      content: data.text,
      metadata: {
        pages: data.numpages,
        info: data.info,
      },
      chunks: pageChunks,
    };
  } catch (error) {
    logger.error('PDF parsing failed', { error });
    throw new Error('Failed to parse PDF document');
  }
}

/**
 * Parse DOCX document
 */
async function parseDOCX(content: ArrayBuffer): Promise<ParsedDocument> {
  try {
    const mammoth = await import('mammoth');
    const result = await mammoth.extractRawText({ arrayBuffer: content });

    const chunks = chunkText(result.value, 'text');

    return {
      content: result.value,
      metadata: {
        messages: result.messages,
      },
      chunks,
    };
  } catch (error) {
    logger.error('DOCX parsing failed', { error });
    throw new Error('Failed to parse DOCX document');
  }
}

/**
 * Parse plain text or markdown
 */
function parseText(content: string, type: 'txt' | 'md'): ParsedDocument {
  const chunks: ParsedChunk[] = [];

  if (type === 'md') {
    // Parse markdown with heading awareness
    const sections = content.split(/(?=^#{1,6}\s)/m);

    for (const section of sections) {
      if (section.trim()) {
        const headingMatch = section.match(/^(#{1,6})\s+(.+)/);
        if (headingMatch && headingMatch[2]) {
          // Add heading as separate chunk
          chunks.push({
            content: headingMatch[2].trim(),
            chunkType: 'heading',
          });
          // Chunk the rest
          const rest = section.slice(headingMatch[0].length).trim();
          if (rest) {
            chunks.push(...chunkText(rest, 'text'));
          }
        } else {
          chunks.push(...chunkText(section, 'text'));
        }
      }
    }
  } else {
    chunks.push(...chunkText(content, 'text'));
  }

  return {
    content,
    metadata: {
      lineCount: content.split('\n').length,
      charCount: content.length,
    },
    chunks,
  };
}

/**
 * Parse CSV file
 */
async function parseCSV(content: string): Promise<ParsedDocument> {
  try {
    const Papa = (await import('papaparse')).default;
    const result = Papa.parse(content, {
      header: true,
      skipEmptyLines: true,
    });

    const chunks: ParsedChunk[] = [];

    // Add header as metadata chunk
    if (result.meta.fields) {
      chunks.push({
        content: `Columns: ${result.meta.fields.join(', ')}`,
        chunkType: 'metadata',
      });
    }

    // Chunk rows into groups
    const rowsPerChunk = 20;
    for (let i = 0; i < result.data.length; i += rowsPerChunk) {
      const rowChunk = result.data.slice(i, i + rowsPerChunk) as unknown[];
      const chunkContent = rowChunk
        .map((row: unknown) => JSON.stringify(row))
        .join('\n');

      chunks.push({
        content: chunkContent,
        chunkType: 'table',
        sourceLineStart: i + 2, // +2 for header and 1-based indexing
        sourceLineEnd: Math.min(i + rowsPerChunk, result.data.length) + 1,
      });
    }

    return {
      content,
      metadata: {
        columns: result.meta.fields,
        rowCount: result.data.length,
      },
      chunks,
    };
  } catch (error) {
    logger.error('CSV parsing failed', { error });
    throw new Error('Failed to parse CSV file');
  }
}

/**
 * Parse JSON file
 */
function parseJSON(content: string): ParsedDocument {
  try {
    const data = JSON.parse(content);
    const formatted = JSON.stringify(data, null, 2);

    // For large JSON, chunk by top-level keys
    const chunks: ParsedChunk[] = [];

    if (typeof data === 'object' && data !== null && !Array.isArray(data)) {
      for (const [key, value] of Object.entries(data)) {
        const valueStr = JSON.stringify(value, null, 2);
        if (valueStr.length > CHUNK_CONFIG.maxChunkSize) {
          // Large value, chunk it
          const subChunks = chunkText(valueStr, 'text');
          subChunks.forEach((chunk, i) => {
            chunks.push({
              ...chunk,
              metadata: { key, part: i + 1 },
            });
          });
        } else {
          chunks.push({
            content: `"${key}": ${valueStr}`,
            chunkType: 'text',
            metadata: { key },
          });
        }
      }
    } else {
      // Array or primitive, just chunk the whole thing
      chunks.push(...chunkText(formatted, 'text'));
    }

    return {
      content: formatted,
      metadata: {
        type: Array.isArray(data) ? 'array' : typeof data,
        keys: typeof data === 'object' && data !== null ? Object.keys(data) : [],
      },
      chunks,
    };
  } catch (error) {
    logger.error('JSON parsing failed', { error });
    throw new Error('Failed to parse JSON file');
  }
}

/**
 * Parse code file
 */
function parseCode(content: string, fileName: string): ParsedDocument {
  const ext = fileName.split('.').pop()?.toLowerCase() || '';
  const lines = content.split('\n');

  const chunks: ParsedChunk[] = [];
  let currentChunk: string[] = [];
  let chunkStart = 1;

  // Detect function/class boundaries for smarter chunking
  const boundaryPatterns = [
    /^(export\s+)?(async\s+)?function\s+\w+/,          // JS/TS functions
    /^(export\s+)?(class|interface|type|enum)\s+\w+/, // JS/TS classes/types
    /^def\s+\w+/,                                      // Python functions
    /^class\s+\w+/,                                    // Python classes
    /^func\s+\w+/,                                     // Go functions
    /^(pub\s+)?fn\s+\w+/,                              // Rust functions
    /^(public|private|protected)?\s*(static\s+)?[\w<>]+\s+\w+\s*\(/,  // Java methods
  ];

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i] ?? '';
    const isNewBlock = boundaryPatterns.some((p) => p.test(line.trim()));

    if (isNewBlock && currentChunk.length >= 5) {
      // Save current chunk and start new one
      chunks.push({
        content: currentChunk.join('\n'),
        chunkType: 'code',
        sourceLineStart: chunkStart,
        sourceLineEnd: i,
      });
      currentChunk = [line];
      chunkStart = i + 1;
    } else if (currentChunk.join('\n').length > CHUNK_CONFIG.codeChunkSize) {
      // Chunk is too big, split it
      chunks.push({
        content: currentChunk.join('\n'),
        chunkType: 'code',
        sourceLineStart: chunkStart,
        sourceLineEnd: i,
      });
      currentChunk = [line];
      chunkStart = i + 1;
    } else {
      currentChunk.push(line);
    }
  }

  // Don't forget the last chunk
  if (currentChunk.length > 0) {
    chunks.push({
      content: currentChunk.join('\n'),
      chunkType: 'code',
      sourceLineStart: chunkStart,
      sourceLineEnd: lines.length,
    });
  }

  return {
    content,
    metadata: {
      language: ext,
      lineCount: lines.length,
      fileName,
    },
    chunks,
  };
}

/**
 * Parse PPTX presentation by extracting text from slide XML
 */
async function parsePPTX(content: ArrayBuffer): Promise<ParsedDocument> {
  try {
    const JSZip = (await import('jszip')).default;
    const zip = await JSZip.loadAsync(content);

    const chunks: ParsedChunk[] = [];
    const allText: string[] = [];

    // Find all slide XML files (slide1.xml, slide2.xml, etc.)
    const slideFiles = Object.keys(zip.files)
      .filter((name) => /^ppt\/slides\/slide\d+\.xml$/.test(name))
      .sort((a, b) => {
        const numA = parseInt(a.match(/slide(\d+)/)?.[1] || '0');
        const numB = parseInt(b.match(/slide(\d+)/)?.[1] || '0');
        return numA - numB;
      });

    for (let i = 0; i < slideFiles.length; i++) {
      const slideFile = slideFiles[i]!;
      const slideXml = await zip.file(slideFile)?.async('string');
      if (!slideXml) continue;

      // Extract text from <a:t> elements
      const textMatches = slideXml.match(/<a:t>([^<]*)<\/a:t>/g) || [];
      const slideTexts = textMatches
        .map((match) => match.replace(/<\/?a:t>/g, '').trim())
        .filter(Boolean);

      const slideContent = slideTexts.join(' ');
      if (slideContent.trim()) {
        allText.push(`Slide ${i + 1}: ${slideContent}`);
        chunks.push({
          content: `Slide ${i + 1}:\n${slideTexts.join('\n')}`,
          chunkType: 'text',
          sourcePage: i + 1,
        });
      }
    }

    const fullText = allText.join('\n\n');

    return {
      content: fullText,
      metadata: {
        slideCount: slideFiles.length,
      },
      chunks: chunks.length > 0 ? chunks : chunkText(fullText, 'text'),
    };
  } catch (error) {
    logger.error('PPTX parsing failed', { error });
    throw new Error('Failed to parse PPTX presentation');
  }
}

/**
 * Parse image file using OpenAI Vision API for OCR
 */
async function parseImage(content: ArrayBuffer): Promise<ParsedDocument> {
  try {
    const buffer = Buffer.from(content);
    const base64 = buffer.toString('base64');

    // Detect MIME type from magic bytes
    let mimeType = 'image/png';
    if (buffer[0] === 0xFF && buffer[1] === 0xD8) mimeType = 'image/jpeg';
    else if (buffer[0] === 0x47 && buffer[1] === 0x49) mimeType = 'image/gif';
    else if (buffer[0] === 0x52 && buffer[1] === 0x49) mimeType = 'image/webp';

    const dataUrl = `data:${mimeType};base64,${base64}`;

    if (!process.env.OPENAI_API_KEY) {
      logger.warn('No OpenAI API key for image OCR, storing with empty chunks');
      return {
        content: '[Image - no OCR available]',
        metadata: { mimeType, ocrAvailable: false },
        chunks: [],
      };
    }

    const { OpenAI } = await import('openai');
    const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

    const response = await client.chat.completions.create({
      model: 'gpt-4.1-mini',
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: 'Extract ALL text content from this image using OCR. Also describe the image contents. Format your response as:\n\n## OCR Text\n[extracted text]\n\n## Description\n[image description]',
            },
            {
              type: 'image_url',
              image_url: { url: dataUrl, detail: 'high' },
            },
          ],
        },
      ],
      max_tokens: 4096,
    });

    const extractedText = response.choices[0]?.message?.content || '';
    const chunks = chunkText(extractedText, 'text');

    return {
      content: extractedText,
      metadata: {
        mimeType,
        ocrModel: 'gpt-4.1-mini',
        ocrAvailable: true,
      },
      chunks,
    };
  } catch (error) {
    logger.error('Image parsing failed', { error });
    // Fallback: store with empty chunks rather than failing entirely
    return {
      content: '[Image - OCR failed]',
      metadata: { ocrAvailable: false, error: error instanceof Error ? error.message : 'Unknown error' },
      chunks: [],
    };
  }
}

/**
 * Parse Excel file
 */
async function parseExcel(content: ArrayBuffer): Promise<ParsedDocument> {
  try {
    const XLSX = await import('xlsx');
    const workbook = XLSX.read(content, { type: 'array' });

    const chunks: ParsedChunk[] = [];
    const allText: string[] = [];

    for (const sheetName of workbook.SheetNames) {
      const sheet = workbook.Sheets[sheetName];
      if (!sheet) continue;
      const csv = XLSX.utils.sheet_to_csv(sheet);
      allText.push(`Sheet: ${sheetName}\n${csv}`);

      // Parse sheet as CSV
      const Papa = (await import('papaparse')).default;
      const result = Papa.parse(csv, {
        header: true,
        skipEmptyLines: true,
      });

      // Add sheet metadata
      chunks.push({
        content: `Sheet "${sheetName}" - Columns: ${result.meta.fields?.join(', ') || 'none'}`,
        chunkType: 'metadata',
        metadata: { sheetName },
      });

      // Chunk rows
      const rowsPerChunk = 20;
      for (let i = 0; i < result.data.length; i += rowsPerChunk) {
        const rowChunk = result.data.slice(i, i + rowsPerChunk) as unknown[];
        chunks.push({
          content: rowChunk.map((row: unknown) => JSON.stringify(row)).join('\n'),
          chunkType: 'table',
          sourceLineStart: i + 2,
          sourceLineEnd: Math.min(i + rowsPerChunk, result.data.length) + 1,
          metadata: { sheetName },
        });
      }
    }

    return {
      content: allText.join('\n\n'),
      metadata: {
        sheetCount: workbook.SheetNames.length,
        sheetNames: workbook.SheetNames,
      },
      chunks,
    };
  } catch (error) {
    logger.error('Excel parsing failed', { error });
    throw new Error('Failed to parse Excel file');
  }
}

/**
 * Chunk text with overlap
 */
function chunkText(text: string, type: ChunkType): ParsedChunk[] {
  const chunks: ParsedChunk[] = [];
  const maxSize = CHUNK_CONFIG.maxChunkSize;
  const overlap = CHUNK_CONFIG.chunkOverlap;

  // Split by paragraphs first
  const paragraphs = text.split(/\n\n+/);
  let currentChunk = '';

  for (const para of paragraphs) {
    const trimmedPara = para.trim();
    if (!trimmedPara) continue;

    if (currentChunk.length + trimmedPara.length > maxSize) {
      if (currentChunk.length >= CHUNK_CONFIG.minChunkSize) {
        chunks.push({
          content: currentChunk.trim(),
          chunkType: type,
        });
        // Keep overlap from the end
        const words = currentChunk.split(/\s+/);
        const overlapWords = words.slice(-Math.ceil(overlap / 5));
        currentChunk = overlapWords.join(' ') + '\n\n' + trimmedPara;
      } else {
        currentChunk += '\n\n' + trimmedPara;
      }
    } else {
      currentChunk += (currentChunk ? '\n\n' : '') + trimmedPara;
    }
  }

  // Don't forget the last chunk
  if (currentChunk.trim().length >= CHUNK_CONFIG.minChunkSize) {
    chunks.push({
      content: currentChunk.trim(),
      chunkType: type,
    });
  } else if (currentChunk.trim() && chunks.length > 0) {
    // Append to last chunk if too small
    const lastChunk = chunks[chunks.length - 1];
    if (lastChunk) {
      lastChunk.content += '\n\n' + currentChunk.trim();
    }
  } else if (currentChunk.trim()) {
    // Only chunk, even if small
    chunks.push({
      content: currentChunk.trim(),
      chunkType: type,
    });
  }

  return chunks;
}

// CJK Unicode ranges for token estimation
const CJK_REGEX = /[\u4e00-\u9fff\u3400-\u4dbf\u{20000}-\u{2a6df}\u{2a700}-\u{2b73f}\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]/u;

/**
 * Estimate token count for a string
 * Uses different ratios based on content type:
 * - English text: ~4 chars/token
 * - Code: ~3.5 chars/token
 * - CJK text: ~1.5 chars/token
 * - JSON/structured: ~3 chars/token
 */
export function estimateTokens(text: string, chunkType?: ChunkType): number {
  if (!text) return 0;

  // Check for CJK content
  const cjkChars = (text.match(CJK_REGEX) || []).length;
  if (cjkChars > text.length * 0.3) {
    // Predominantly CJK text
    const cjkTokens = cjkChars / 1.5;
    const otherTokens = (text.length - cjkChars) / 4;
    return Math.ceil(cjkTokens + otherTokens);
  }

  // Content-type based estimation
  if (chunkType === 'code') {
    return Math.ceil(text.length / 3.5);
  }

  // Check for JSON/structured content
  if (chunkType === 'table' || chunkType === 'metadata') {
    return Math.ceil(text.length / 3);
  }

  // Default: English text
  return Math.ceil(text.length / 4);
}
